{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-1 MNIST - softmax\n",
    "     - Adam : SGD 대신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: MNIST_data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.2901,  0.3930, -0.1891,  ..., -0.1626, -0.3275,  0.5608],\n",
       "        [-1.6757, -0.2454,  0.9221,  ...,  0.9559,  0.6160, -0.4226],\n",
       "        [ 0.2444,  1.4810, -2.0906,  ...,  0.1206,  1.0044, -0.3737],\n",
       "        ...,\n",
       "        [ 0.6234,  1.8019, -2.7563,  ..., -0.5889, -0.5576,  0.7360],\n",
       "        [-0.2871, -1.3313, -2.2248,  ...,  0.0309,  0.9180, -0.1482],\n",
       "        [ 0.7678,  0.6624, -0.5362,  ...,  0.2338,  0.3688, -0.7182]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.normal_(linear.weight) # 정규분포, 784 x 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam : gradient, learning rate 둘다 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e9b1d5bf4f496e8efb333aaf16fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 5.656054974\n",
      "Epoch: 0002 cost = 1.699803472\n",
      "Epoch: 0003 cost = 1.121566176\n",
      "Epoch: 0004 cost = 0.883357942\n",
      "Epoch: 0005 cost = 0.750486851\n",
      "Epoch: 0006 cost = 0.663246810\n",
      "Epoch: 0007 cost = 0.601343870\n",
      "Epoch: 0008 cost = 0.554267764\n",
      "Epoch: 0009 cost = 0.518746793\n",
      "Epoch: 0010 cost = 0.489619613\n",
      "Epoch: 0011 cost = 0.465846598\n",
      "Epoch: 0012 cost = 0.446371138\n",
      "Epoch: 0013 cost = 0.429083288\n",
      "Epoch: 0014 cost = 0.414352983\n",
      "Epoch: 0015 cost = 0.401432723\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in tqdm(range(training_epochs)):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader: # 60000\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device) # 60000 x 784\n",
    "        Y = Y.to(device) # 60000\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%02d' % (epoch + 1), '\\t','cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test.test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8905999660491943\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device) # 10000 x 784\n",
    "    Y_test = mnist_test.test_labels.to(device) # 10000\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-2 MNIST - NN\n",
    "    - Adam\n",
    "    - ReLU : sigmoid 대신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReLU : Rectified Linear Unit\n",
    "    - sigmoid 를 사용하면 층이 깊어질수록 vanishing gradient 문제가 발생하게 된다. \n",
    "  ![title](Re.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-3.1874,  0.7429, -0.6153,  ..., -1.8201,  0.1607, -0.1961],\n",
       "        [ 0.3986, -0.8667, -0.5681,  ...,  0.0619, -2.2653, -0.2265],\n",
       "        [-1.2663,  0.8911, -0.4557,  ...,  1.8710,  0.6392, -0.3189],\n",
       "        ...,\n",
       "        [ 0.6468, -0.6644, -0.6639,  ...,  1.1450, -1.2163,  0.2803],\n",
       "        [-1.0293,  0.2693,  0.8209,  ..., -0.3936, -0.7411,  1.0513],\n",
       "        [ 3.1485,  2.3720,  2.9838,  ...,  1.5475,  0.1471, -1.6560]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bbe018dbf24382b03a4937a4287407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 35.101280212\n",
      "Epoch: 0002 cost = 22.568840027\n",
      "Epoch: 0003 cost = 15.889390945\n",
      "Epoch: 0004 cost = 11.665189743\n",
      "Epoch: 0005 cost = 8.612565041\n",
      "Epoch: 0006 cost = 6.470128536\n",
      "Epoch: 0007 cost = 4.887979031\n",
      "Epoch: 0008 cost = 3.641815424\n",
      "Epoch: 0009 cost = 2.760086536\n",
      "Epoch: 0010 cost = 2.111594439\n",
      "Epoch: 0011 cost = 1.742623687\n",
      "Epoch: 0012 cost = 1.322564125\n",
      "Epoch: 0013 cost = 1.141616702\n",
      "Epoch: 0014 cost = 0.857270122\n",
      "Epoch: 0015 cost = 0.778423071\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in tqdm(range(training_epochs)):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        \n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9498999714851379\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-3 : MNIST - NN\n",
    "    - Adam\n",
    "    - ReLU\n",
    "    - xavier initialization : 정규분포 대신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xvarier initialization : 가중치 초기화 방법 중 하나로 이전노드와 다음노드 개수에 의존한다.\n",
    "    - 방법은 uniform, normal 두 가지가 있다.\n",
    "   ![title](x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0598,  0.1128, -0.1468,  ..., -0.0484,  0.0354, -0.1161],\n",
       "        [-0.0187,  0.1168,  0.0382,  ...,  0.0520, -0.1378,  0.0129],\n",
       "        [-0.0869, -0.1015, -0.0170,  ...,  0.1181,  0.0025,  0.0385],\n",
       "        ...,\n",
       "        [-0.1128, -0.0081,  0.0624,  ..., -0.0698,  0.0839, -0.0214],\n",
       "        [ 0.0328,  0.0814,  0.1145,  ..., -0.0209, -0.1137,  0.0286],\n",
       "        [-0.1479, -0.0680, -0.0628,  ...,  0.1377, -0.0184,  0.1055]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204ae59196944b788ef65af259c7b234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.245592877\n",
      "Epoch: 0002 cost = 0.093776651\n",
      "Epoch: 0003 cost = 0.061814509\n",
      "Epoch: 0004 cost = 0.045181893\n",
      "Epoch: 0005 cost = 0.033410612\n",
      "Epoch: 0006 cost = 0.027215190\n",
      "Epoch: 0007 cost = 0.021980770\n",
      "Epoch: 0008 cost = 0.017516701\n",
      "Epoch: 0009 cost = 0.016731752\n",
      "Epoch: 0010 cost = 0.013409850\n",
      "Epoch: 0011 cost = 0.014096180\n",
      "Epoch: 0012 cost = 0.011319625\n",
      "Epoch: 0013 cost = 0.010022114\n",
      "Epoch: 0014 cost = 0.010152928\n",
      "Epoch: 0015 cost = 0.013662727\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "for epoch in tqdm(range(training_epochs)):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9788999557495117\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-4 MNIST - NN deep + dropout\n",
    "    1. Adam\n",
    "    2. ReLU\n",
    "    3. xavier initialization\n",
    "    4. 층을 늘리고 계산도 늘어남 : 256 -> 512\n",
    "    5. dropout : overffiting 방지\n",
    "   ![title](d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0347, -0.0152,  0.0010,  ...,  0.0278,  0.0165,  0.0407],\n",
       "        [-0.0550, -0.0439, -0.0726,  ...,  0.0637, -0.0342,  0.0099],\n",
       "        [-0.0107, -0.0572, -0.0919,  ...,  0.0434, -0.1031, -0.0155],\n",
       "        ...,\n",
       "        [ 0.0998,  0.0770,  0.0183,  ..., -0.0682,  0.0101,  0.0966],\n",
       "        [-0.1035, -0.0221,  0.0700,  ..., -0.0634,  0.0604,  0.0396],\n",
       "        [-0.0090, -0.0855,  0.0824,  ..., -0.0219, -0.0786,  0.0055]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, relu, dropout,\n",
    "                            linear2, relu, dropout,\n",
    "                            linear3, relu, dropout,\n",
    "                            linear4, relu, dropout,\n",
    "                            linear5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002f5693379f4a9982a8c370c6d28fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.310551524\n",
      "Epoch: 0002 cost = 0.142300949\n",
      "Epoch: 0003 cost = 0.113023162\n",
      "Epoch: 0004 cost = 0.094208725\n",
      "Epoch: 0005 cost = 0.082151912\n",
      "Epoch: 0006 cost = 0.074239850\n",
      "Epoch: 0007 cost = 0.065893605\n",
      "Epoch: 0008 cost = 0.060688630\n",
      "Epoch: 0009 cost = 0.058843244\n",
      "Epoch: 0010 cost = 0.051878437\n",
      "Epoch: 0011 cost = 0.054459579\n",
      "Epoch: 0012 cost = 0.050582249\n",
      "Epoch: 0013 cost = 0.046774726\n",
      "Epoch: 0014 cost = 0.047748785\n",
      "Epoch: 0015 cost = 0.040831063\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_batch = len(data_loader)\n",
    "model.train()    # set the model to train mode (dropout=True)\n",
    "for epoch in tqdm(range(training_epochs)):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        \n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979699969291687\n",
      "Label:  4\n",
      "Prediction:  4\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()    # set the model to evaluation mode (dropout=False)\n",
    "\n",
    "    # Test the model using test sets\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST - batch normalization\n",
    "    - batch normalization vs. NN\n",
    "    - gradient vanishing, gradient exploding 해결\n",
    "    - internal covatiate shift -> batch normalization\n",
    "   ![title](b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(784, 32, bias=True)\n",
    "linear2 = torch.nn.Linear(32, 32, bias=True)\n",
    "linear3 = torch.nn.Linear(32, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "bn1 = torch.nn.BatchNorm1d(32)\n",
    "bn2 = torch.nn.BatchNorm1d(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_linear1 = torch.nn.Linear(784, 32, bias=True)\n",
    "nn_linear2 = torch.nn.Linear(32, 32, bias=True)\n",
    "nn_linear3 = torch.nn.Linear(32, 10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = torch.nn.Sequential(linear1, bn1, relu,\n",
    "                            linear2, bn2, relu,\n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = torch.nn.Sequential(nn_linear1, relu,\n",
    "                               nn_linear2, relu,\n",
    "                               nn_linear3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)   \n",
    "bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n",
    "nn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "train_total_batch = len(train_loader)\n",
    "test_total_batch = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1416067b7146ddb45029071e20b532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.12375(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.16215(nn_acc:0.95)\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.13067(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.17032(nn_acc:0.95)\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.10562(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.20314(nn_acc:0.94)\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.12786(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.23511(nn_acc:0.94)\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07909(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.15494(nn_acc:0.95)\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.10587(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18454(nn_acc:0.95)\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.07014(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12172(nn_acc:0.96)\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.09700(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.16621(nn_acc:0.95)\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06708(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.14064(nn_acc:0.96)\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.09369(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18000(nn_acc:0.95)\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06074(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.10974(nn_acc:0.97)\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.09732(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.16380(nn_acc:0.96)\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05411(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12802(nn_acc:0.96)\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.09001(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19530(nn_acc:0.95)\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05492(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.11704(nn_acc:0.97)\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.09320(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18562(nn_acc:0.96)\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05463(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.09910(nn_acc:0.97)\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.09848(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.16869(nn_acc:0.96)\n",
      "\n",
      "[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05153(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.11413(nn_acc:0.97)\n",
      "[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.09575(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18205(nn_acc:0.96)\n",
      "\n",
      "\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(training_epochs)):\n",
    "    bn_model.train()  # set the model to train mode\n",
    "\n",
    "    for X, Y in train_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # label is not one-hot encoded\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        bn_optimizer.zero_grad()\n",
    "        bn_prediction = bn_model(X)\n",
    "        bn_loss = criterion(bn_prediction, Y)\n",
    "        bn_loss.backward()\n",
    "        bn_optimizer.step()\n",
    "\n",
    "        nn_optimizer.zero_grad()\n",
    "        nn_prediction = nn_model(X)\n",
    "        nn_loss = criterion(nn_prediction, Y)\n",
    "        nn_loss.backward()\n",
    "        nn_optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bn_model.eval()     # set the model to evaluation mode\n",
    "\n",
    "        # Test the model using train sets\n",
    "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "            X = X.view(-1, 28 * 28).to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            bn_prediction = bn_model(X)\n",
    "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
    "            bn_loss += criterion(bn_prediction, Y)\n",
    "            bn_acc += bn_correct_prediction.float().mean()\n",
    "\n",
    "            nn_prediction = nn_model(X)\n",
    "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
    "            nn_loss += criterion(nn_prediction, Y)\n",
    "            nn_acc += nn_correct_prediction.float().mean()\n",
    "\n",
    "        bn_loss, nn_loss  = bn_loss / train_total_batch, nn_loss / train_total_batch\n",
    "        bn_acc, nn_acc = bn_acc / train_total_batch, nn_acc / train_total_batch\n",
    "        \n",
    "        # Save train losses/acc\n",
    "        train_losses.append([bn_loss, nn_loss])\n",
    "        train_accs.append([bn_acc, nn_acc])\n",
    "        print(\n",
    "            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
    "            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
    "        \n",
    "        # Test the model using test sets\n",
    "        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\n",
    "        \n",
    "        for i, (X, Y) in enumerate(test_loader):\n",
    "            X = X.view(-1, 28 * 28).to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            bn_prediction = bn_model(X)\n",
    "            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\n",
    "            bn_loss += criterion(bn_prediction, Y)\n",
    "            bn_acc += bn_correct_prediction.float().mean()\n",
    "\n",
    "            nn_prediction = nn_model(X)\n",
    "            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\n",
    "            nn_loss += criterion(nn_prediction, Y)\n",
    "            nn_acc += nn_correct_prediction.float().mean()\n",
    "\n",
    "        bn_loss, nn_loss = bn_loss / test_total_batch, nn_loss / test_total_batch\n",
    "        bn_acc, nn_acc = bn_acc / test_total_batch, nn_acc / test_total_batch\n",
    "\n",
    "        # Save valid losses/acc\n",
    "        valid_losses.append([bn_loss, nn_loss])\n",
    "        valid_accs.append([bn_acc, nn_acc])\n",
    "        print(\n",
    "            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\n",
    "                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\n",
    "        print()\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plt 사용하여 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n",
    "    bn = [i[0] for i in loss_list]\n",
    "    nn = [i[1] for i in loss_list]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(bn, label='With BN')\n",
    "    plt.plot(nn, label='Without BN')\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "plot_compare(train_losses, title='Training Loss at Epoch')\n",
    "plot_compare(train_accs, [0, 1.0], title='Training Acc at Epoch')\n",
    "# validation\n",
    "plot_compare(valid_losses, title='Validation Loss at Epoch')\n",
    "plot_compare(valid_accs, [0, 1.0], title='Validation Acc at Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
