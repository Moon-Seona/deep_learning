{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 8-4 MNIST - back propagation\n",
    "   \n",
    "       - back propagation 직접 구현\n",
    "       - training 하면서 tensor에 계속 할당하면서 gpu 메모리 소모 -> training 줄임ㅠㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w : weight, b : bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.nn.Parameter(torch.Tensor(784, 30)).to(device)\n",
    "b1 = torch.nn.Parameter(torch.Tensor(30)).to(device)\n",
    "w2 = torch.nn.Parameter(torch.Tensor(30, 10)).to(device)\n",
    "b2 = torch.nn.Parameter(torch.Tensor(10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1009, -0.2272, -0.1022, -0.9369,  0.6552, -0.8914,  0.1989,  0.7264,\n",
       "         0.3932, -0.5963], device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.normal_(w1)\n",
    "torch.nn.init.normal_(b1)\n",
    "torch.nn.init.normal_(w2)\n",
    "torch.nn.init.normal_(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: MNIST_data/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 60000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.zeros(60000,784).to(device)\n",
    "a1 = torch.matmul(X, w1)\n",
    "torch.transpose(a1, 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moon\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\Moon\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  \t 725\n",
      "accuracy :  \t 754\n",
      "accuracy :  \t 790\n",
      "accuracy :  \t 796\n",
      "accuracy :  \t 875\n",
      "accuracy :  \t 893\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 3.00 GiB total capacity; 977.96 MiB already allocated; 0 bytes free; 978.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9a2a18ebfc9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# bias 도 개선\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_w1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mw2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_w2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 3.00 GiB total capacity; 977.96 MiB already allocated; 0 bytes free; 978.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# mnist_test.test_data : 10000 x 28 x 28\n",
    "# mnist_test.test_data.view(-1, 28 * 28) : 10000 x 784\n",
    "# mnist_test.test_data.view(-1, 28 * 28).float().to(device)[:1000] : 1000 x 784\n",
    "\n",
    "X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)[:1000]\n",
    "\n",
    "# mnist_test.test_labels : 10000\n",
    "# mnist_test.test_labels.to(device)[:1000] : 1000\n",
    "\n",
    "Y_test = mnist_test.test_labels.to(device)[:1000]\n",
    "\n",
    "i = 0\n",
    "while not i == 1000:\n",
    "\n",
    "    for X, Y in data_loader: # 60000\n",
    "        i += 1\n",
    "\n",
    "        # forward propagation\n",
    "        X = X.view(-1, 28 * 28).to(device) # 60000 x 784\n",
    "        Y = torch.zeros((batch_size, 10)).scatter_(1, Y.unsqueeze(1), 1).to(device)    # one-hot, 10 x 10\n",
    "        \n",
    "        l1 = torch.add(torch.matmul(X, w1), b1) # 60000 x 784 @ 784 x 30 = 60000 x 30 + bias\n",
    "        a1 = sigmoid(l1)\n",
    "        \n",
    "        l2 = torch.add(torch.matmul(a1, w2), b2) # 60000 x 30 @ 30 x 10 = 60000 x 10 + bias\n",
    "        y_pred = sigmoid(l2) # 60000 x 10\n",
    "\n",
    "        diff = y_pred - Y\n",
    "\n",
    "        # Back prop (chain rule)\n",
    "        d_l2 = diff * sigmoid_prime(l2)\n",
    "        d_b2 = d_l2\n",
    "        \n",
    "        d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_l2) # 전치, 30 x 60000 @ 60000 x 10 = 30 x 10\n",
    "\n",
    "        d_a1 = torch.matmul(d_l2, torch.transpose(w2, 0, 1)) # 60000 x 10 @ 10 x 30 = 60000 x 30\n",
    "        d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "        d_b1 = d_l1\n",
    "        d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_l1) # 784 x 60000 @ 60000 x 10 = 784 x 10\n",
    "\n",
    "        # bias 도 개선\n",
    "        w1 = w1 - learning_rate * d_w1\n",
    "        b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "        w2 = w2 - learning_rate * d_w2\n",
    "        b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            l1 = torch.add(torch.matmul(X_test, w1), b1)\n",
    "            a1 = sigmoid(l1)\n",
    "            l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "            y_pred = sigmoid(l2)\n",
    "            acct_mat = torch.argmax(y_pred, 1) == Y_test\n",
    "            acct_res = acct_mat.sum()\n",
    "            print(\"accuracy : \", \"\\t\", acct_res.item())\n",
    "\n",
    "        if i == 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
